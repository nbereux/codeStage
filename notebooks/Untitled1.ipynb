{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/nicolas/Stage/code/stage/src')\n",
    "sys.path.insert(1, '/home/nicolas/Stage/code/stage/data')\n",
    "sys.path.insert(1, '/home/nicolas/Stage/code/stage/model')\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from ComparisonDataset import ComparisonDataset\n",
    "from BinaryClassifier import BinaryClassifier, train, test\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import functions\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float\n",
    "\n",
    "fname = \"AllParameters_LongRUNExMC_nMB500_NS50000_TEMP_0_MNIST_Nh500_lr0.01_l20.0_NGibbs10000.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('./data/mnist.pkl.gz', 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "p = u.load()\n",
    "train_set, _, _ = p\n",
    "\n",
    "data_mnist = torch.as_tensor(\n",
    "    train_set[0][:10000, :].T, device=device, dtype=dtype)\n",
    "labels_mnist = torch.tensor(np.zeros(len(data_mnist[0])), device=device)\n",
    "\n",
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "cuda_kwargs = {'num_workers': 0,\n",
    "               'pin_memory': False,\n",
    "               'shuffle': False}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-67b609f1dce8>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_gen = torch.tensor(si, device=device)\n",
      "/home/nicolas/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1622099212932/work/c10/core/TensorImpl.h:1260.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'success_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-67b609f1dce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         tmp.append(torch.linalg.norm(net(test_set.data[:2500].cuda()).round().view(\n\u001b[1;32m     45\u001b[0m             2500)-test_set.label[:2500].cuda(), 1).item()*100/2500)\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msuccess_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'success_rate' is not defined"
     ]
    }
   ],
   "source": [
    "fname = \"model/\" + fname\n",
    "myRBM, f, alltimes = functions.retrieveRBM(device, fname)\n",
    "\n",
    "times = np.array(alltimes)[range(len(alltimes))]\n",
    "tmp = []\n",
    "for t in [times[-1]]:\n",
    "    myRBM.W = torch.tensor(f['paramW'+str(t)], device=myRBM.device)\n",
    "    myRBM.vbias = torch.tensor(\n",
    "        f['paramVB'+str(t)], device=myRBM.device)\n",
    "    myRBM.hbias = torch.tensor(\n",
    "        f['paramHB'+str(t)], device=myRBM.device)\n",
    "    vinit = torch.bernoulli(torch.rand(\n",
    "        (myRBM.Nv, 10000), device=myRBM.device, dtype=myRBM.dtype))\n",
    "\n",
    "    si, _, _, _ = myRBM.Sampling(vinit, it_mcmc=10000)\n",
    "    data_gen = torch.tensor(si, device=device)\n",
    "    labels_gen = torch.tensor(np.ones(len(data_gen[0])), device=device)\n",
    "    data = torch.cat((data_gen, data_mnist), dim=1)\n",
    "    labels = torch.cat((labels_gen, labels_mnist), dim=0)\n",
    "    data = data.T\n",
    "    shuffle_index = torch.randperm(data.shape[0])\n",
    "    newdata = torch.empty(data.shape)\n",
    "    newlabel = torch.empty(labels.shape)\n",
    "    for i in range(shuffle_index.shape[0]):\n",
    "        newdata[i] = data[shuffle_index[i]]\n",
    "        newlabel[i] = labels[shuffle_index[i]]\n",
    "\n",
    "    net = BinaryClassifier()\n",
    "    net.cuda()\n",
    "    train_set = ComparisonDataset(\n",
    "        device, data=newdata, labels=newlabel, train=True)\n",
    "    test_set = ComparisonDataset(\n",
    "        device, data=newdata, labels=newlabel, train=False)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, **test_kwargs)\n",
    "    optimizer = optim.Adadelta(net.parameters(), lr=0.1)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "    for epoch in range(1, 100 + 1):\n",
    "        train(net, device, train_loader, optimizer, epoch)\n",
    "        test(net, device, test_loader)\n",
    "        scheduler.step()\n",
    "        tmp.append(torch.linalg.norm(net(test_set.data[:2500].cuda()).round().view(\n",
    "            2500)-test_set.label[:2500].cuda(), 1).item()*100/2500)\n",
    "    success_rate.append(tmp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.8,\n",
       " 46.48,\n",
       " 46.64,\n",
       " 46.52,\n",
       " 46.44,\n",
       " 46.56,\n",
       " 46.48,\n",
       " 45.88,\n",
       " 45.84,\n",
       " 45.72,\n",
       " 45.92,\n",
       " 45.76,\n",
       " 45.84,\n",
       " 45.88,\n",
       " 45.72,\n",
       " 45.76,\n",
       " 45.76,\n",
       " 45.76,\n",
       " 45.72,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68,\n",
       " 45.68]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
