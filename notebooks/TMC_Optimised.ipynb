{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "sys.path.insert(1, '/home/nicolas/code/src')\n",
    "sys.path.insert(1, '/home/nicolas/code/data')\n",
    "\n",
    "from RBM import RBM\n",
    "from scipy.integrate import simps\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMCSample(v, w_hat, N, V, it_mcmc=100, it_mean=50, ß=1):\n",
    "    #print(\"Initialisation\")\n",
    "    #s = time.time()\n",
    "    vtab = torch.zeros(v.shape, device = device)\n",
    "    v_curr = v\n",
    "    #V = V\n",
    "    norm = 1/(v_curr.shape[0]**0.5)\n",
    "    w_curr = (torch.mv(v_curr.T, V)*norm)\n",
    "    \n",
    "    index = torch.randperm(v_curr.shape[0])\n",
    "    #print(time.time()-s)\n",
    "    #print(\"IT MCMC\")\n",
    "    s = time.time()\n",
    "\n",
    "    for t in range(it_mcmc):\n",
    "        #print('init it')\n",
    "        print(t)\n",
    "        h_curr, _ = myRBM.SampleHiddens01(v_curr)\n",
    "        h_i = (torch.mm(myRBM.W.T, h_curr)+myRBM.vbias.reshape(v.shape[0],1)) # Nv x Ns\n",
    "        w_next = w_curr.clone()\n",
    "        \n",
    "        v_next = torch.clone(v_curr)\n",
    "        index = torch.randperm(v_curr.shape[0])\n",
    "        for idx in range(v_curr.shape[0]):\n",
    "            #print('upd comp')\n",
    "            s = time.time()\n",
    "            i = idx\n",
    "            v_next[i,:] = 1-v_curr[i,:]\n",
    "            w_next += ((2*v_next[i,:]-1)*V[i]*norm)\n",
    "\n",
    "            # On calcul -DeltaE\n",
    "            ΔE = ß*((2*v_next[i,:]-1)*h_i[i,:])-(N/2)*((w_hat-w_next)**2-(w_hat-w_curr)**2)\n",
    "\n",
    "            tir = torch.rand(v_curr.shape[1],1, device = torch.device(\"cuda\")).squeeze()\n",
    "            prob = torch.exp(ΔE).squeeze()\n",
    "            v_curr[i,:] = torch.where(tir<prob, v_next[i,:], v_curr[i,:])\n",
    "            v_next[i,:] = torch.where(tir<prob, v_next[i,:], 1-v_next[i,:])\n",
    "            w_curr = torch.where(tir<prob, w_next, w_curr)\n",
    "            w_next = torch.where(tir<prob, w_next, w_curr)\n",
    "            #print(time.time()-s)\n",
    "        if (t>= (it_mcmc-it_mean)):\n",
    "            vtab += v_curr\n",
    "    print(time.time()-s)\n",
    "\n",
    "    vtab = vtab*(1/it_mean)    \n",
    "    return v_curr, h_curr, vtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMCSampleTEST(v, w_hat, N, V,it_mcmc=100,it_mean=50, ß=1):\n",
    "    vtab = torch.zeros(v.shape, device = device)\n",
    "    v_curr = v\n",
    "    V = V\n",
    "    norm = 1/(v_curr.shape[0]**0.5)\n",
    "    w_curr = (torch.mv(v_curr.T, V)*norm)\n",
    "    \n",
    "    index = torch.randperm(v_curr.shape[0])\n",
    "    tmp = []\n",
    "\n",
    "    for t in range(it_mcmc):\n",
    "        #print('init it')\n",
    "        print(t)\n",
    "        h_curr, _ = myRBM.SampleHiddens01(v_curr)\n",
    "        h_i = (torch.mm(myRBM.W.T, h_curr)+myRBM.vbias.reshape(v.shape[0],1)) # Nv x Ns\n",
    "        w_next = w_curr.clone()\n",
    "        \n",
    "        v_next = torch.clone(v_curr)\n",
    "        index = torch.randperm(v_curr.shape[0])\n",
    "        for idx in range(v_curr.shape[0]):\n",
    "            #print('upd comp')\n",
    "            i = idx\n",
    "            v_next[i,:] = 1-v_curr[i,:]\n",
    "            w_next += ((2*v_next[i,:]-1)*V[i]*norm)\n",
    "\n",
    "            # On calcul -DeltaE\n",
    "            ΔE = ß*((2*v_next[i,:]-1)*h_i[i,:])-(N/2)*((w_hat-w_next)**2-(w_hat-w_curr)**2)\n",
    "\n",
    "            tir = torch.rand(v_curr.shape[1],1, device = torch.device(\"cuda\")).squeeze()\n",
    "            prob = torch.exp(ΔE).squeeze()\n",
    "            index_T = torch.where(tir<prob)\n",
    "            v_curr[i,index] = v_next[i, index]\n",
    "            v_next = v_curr.clone()\n",
    "            w_curr[index] = w_next[index]\n",
    "            w_next = w_curr.clone()\n",
    "            \n",
    "            \n",
    "            #v_curr[i,:] = torch.where(tir<prob, v_next[i,:], v_curr[i,:])\n",
    "            #v_next[i,:] = torch.where(tir<prob, v_next[i,:], 1-v_next[i,:])\n",
    "            #w_curr = torch.where(tir<prob, w_next, w_curr)\n",
    "            #w_next = torch.where(tir<prob, w_next, w_curr)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(time.time()-s)\n",
    "        if (t>= (it_mcmc-it_mean)):\n",
    "            vtab += v_curr\n",
    "        tmp.append(torch.mean(v_curr[:,4000*15])-0.4)\n",
    "    plt.plot(tmp)\n",
    "    vtab = vtab*(1/it_mean)    \n",
    "    return v_curr, h_curr, vtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "W = np.genfromtxt('../data/C1d5c/rbm_W.dat').T\n",
    "vbias = np.genfromtxt('../data/C1d5c/rbm_vis.dat')\n",
    "hbias = -np.genfromtxt('../data/C1d5c/rbm_hid.dat')\n",
    "data = np.genfromtxt('../data/C1d5c/data_5.dat')\n",
    "data = (data+1)/2\n",
    "\n",
    "lr = 0.01\n",
    "l2 = 0\n",
    "NGibbs = 10\n",
    "annSteps = 0\n",
    "mb_s = 500\n",
    "num_pcd = 500\n",
    "Nh = W.shape[0]\n",
    "Nv = W.shape[1]\n",
    "print(Nh)\n",
    "ep_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRBM = RBM(num_visible=Nv,\n",
    "\t\t\t\tnum_hidden=Nh,\n",
    "\t\t\t\tdevice=device,\n",
    "\t\t\t\tlr=lr,\n",
    "\t\t\t\t#regL2=l2,\n",
    "\t\t\t\tgibbs_steps=NGibbs,\n",
    "\t\t\t\t#anneal_steps=annSteps,\n",
    "\t\t\t\tUpdCentered=True,\n",
    "\t\t\t\tmb_s=mb_s,\n",
    "\t\t\t\tnum_pcd=num_pcd)\n",
    "\n",
    "myRBM.W = torch.tensor(4*W).float().cuda()\n",
    "myRBM.vbias = torch.tensor(2*vbias - 2*W.sum(0)).float().to(device)\n",
    "myRBM.hbias = torch.tensor(2*hbias - 2*W.sum(1)).float().to(device)\n",
    "_, _, V = torch.svd(torch.tensor(4*W).float().cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, V_g = torch.svd(myRBM.W)\n",
    "proj_data =torch.mm(torch.tensor(data, device= device, dtype = dtype), V_g).cpu()/myRBM.Nv**0.5\n",
    "plt.hist(proj_data[:,0].cpu().numpy(), bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_points = torch.bernoulli(torch.rand(myRBM.Nv, 1000, device = device))\n",
    "#arrival,_,_,_ = myRBM.Sampling(start_points, it_mcmc = 1000)\n",
    "#proj_gen = torch.mm(arrival.T, V).cpu()/myRBM.Nv**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_chain = 10 # Nb de chaines pour chaque w_hat\n",
    "it_mcmc = 100 # Nb it_mcmc pour chaque chaine\n",
    "it_mean = 150 # Nb it considérée pour la moyenne temporelle de chaque chaine\n",
    "N = 20000 # Contrainte\n",
    "nb_point = 15000 # Nb de points de discrétisation pour w_hat\n",
    "xmin = -0.0 \n",
    "xmax = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "0.0003604888916015625\n"
     ]
    }
   ],
   "source": [
    "start = torch.bernoulli(torch.rand(myRBM.Nv, nb_chain*nb_point, device = device))\n",
    "V0 = V[:,0]\n",
    "# w_hat = torch.dot(start.T, V)[0:,]\n",
    "w_hat_b = torch.linspace(xmin, xmax, steps =nb_point, device = device)\n",
    "w_hat = torch.zeros(nb_chain*nb_point, device = device)\n",
    "for i in range(nb_point):\n",
    "    for j in range(nb_chain):\n",
    "        w_hat[i*nb_chain+j] = w_hat_b[i]\n",
    "tmpv, tmph, vtab = TMCSample(start, w_hat, N, V0, it_mcmc = it_mcmc, it_mean=it_mean)\n",
    "\n",
    "y = np.array(torch.mm(vtab.T, V0.unsqueeze(1)).cpu().squeeze())/myRBM.Nv**0.5\n",
    "newy = np.array([np.mean(y[i*nb_chain:i*nb_chain+nb_chain]) for i in range(nb_point)])\n",
    "w_hat = w_hat.cpu().numpy()\n",
    "w_hat_b = w_hat_b.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 150000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "res=np.zeros(len(w_hat_b)-1)\n",
    "for i in range(1,len(w_hat_b)):\n",
    "    res[i-1] = simps(newy[:i]-w_hat_b[:i], w_hat_b[:i])\n",
    "const = simps(np.exp(N*res), w_hat_b[:-1])\n",
    "p_m = np.exp(N*res)/const\n",
    "print(simps(p_m, w_hat_b[:-1]))\n",
    "potential = res + (1/N)*np.log(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9258923c4d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproj_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmyRBM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNv\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tab:red'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w_hat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "proj_data =torch.mm(torch.tensor(data, device = device, dtype = dtype), V).cpu()/myRBM.Nv**0.5\n",
    "fig, ax1 = plt.subplots(dpi = 200)\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel(\"w_hat\")\n",
    "ax1.plot(w_hat_b,newy-w_hat_b,color='red', label = \"grad potential\")\n",
    "ax1.hlines(0,0,1, color = 'black')\n",
    "#ax1.scatter(proj_gen[:,0], proj_gen[:,1],alpha=0.2, label = 'data_gen')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.plot(w_hat_b[1:], potential, label = \"potential\")\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "rdm_y = torch.randn(proj_data[:,0].shape)/20\n",
    "ax2.hist(proj_data[:,0].numpy(), label = 'data', density=True, bins=200)\n",
    "ax2.plot(w_hat_b[1:], p_m, color = \"green\", label = \"prob\")\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.savefig(\"../../Stage/rapport/fig/TMC_Example.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"TMC_Example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
